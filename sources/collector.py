import os
import json
import random
from datetime import datetime, timedelta, timezone, date
from pathlib import Path
import math

from core.logger import log
from sources.rss import fetch_rss
from utils.helpers import generate_id, fetch_main_image, download_image




# === –ù–ê–°–¢–†–û–ô–ö–ò ===
DATA_DIR = Path(os.getenv("DATA_DIR", "data"))
IMG_DIR = DATA_DIR / "images"
NEWS_PATH = DATA_DIR / "news.json"
SCHEDULE_FILE = Path("data/schedule.json")

RSS_SOURCES = [
    "https://www.theverge.com/rss/index.xml",
    "https://www.wired.com/feed/rss",
    "https://techcrunch.com/feed/",
    "https://feeds.arstechnica.com/arstechnica/index",
    "https://www.engadget.com/rss.xml",
    "https://venturebeat.com/feed/",
    "https://github.blog/feed/",
    "https://stackoverflow.blog/feed/",
    "https://feeds.feedburner.com/TheHackersNews",
    "https://www.analyticsvidhya.com/blog/feed/",
    "https://www.marktechpost.com/feed/",
    "https://thenewstack.io/feed/",
    "https://towardsdatascience.com/feed",
    "https://ai.googleblog.com/feeds/posts/default",
    "https://openai.com/blog/rss",

]

def build_schedule(news_count, start_hour=5, end_hour=22):
    """–°–æ–∑–¥–∞—ë—Ç —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –ø–æ—Å—Ç–∏–Ω–≥–∞ –Ω–∞ –¥–µ–Ω—å."""
    start = datetime.combine(datetime.today(), datetime.min.time()).replace(hour=start_hour)
    total_minutes = (end_hour - start_hour) * 60
    if news_count == 0:
        return []
    interval = total_minutes / news_count

    times = [start + timedelta(minutes=i * interval) for i in range(news_count)]
    schedule = [t.strftime("%H:%M") for t in times]

    SCHEDULE_FILE.write_text(json.dumps(schedule, indent=2), encoding="utf-8")
    log.info(f"üïí –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –∏–∑ {len(schedule)} –ø—É–±–ª–∏–∫–∞—Ü–∏–π –∫–∞–∂–¥—ã–µ {interval:.1f} –º–∏–Ω.")
    return schedule

def load_existing_ids():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —É–∂–µ —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ (–¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç –¥—É–±–ª–µ–π)."""
    if NEWS_PATH.exists():
        try:
            with open(NEWS_PATH, "r", encoding="utf-8") as f:
                data = json.load(f)
                return {item["id"] for item in data.get("items", [])}
        except Exception as e:
            log.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {NEWS_PATH}: {e}")
    return set()


def safe_fetch_image(url):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç —Å—Å—ã–ª–∫—É –Ω–∞ –≥–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ."""
    try:
        return fetch_main_image(url)
    except Exception as e:
        log.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        return None


def is_today(published_at):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ –¥–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∫ —Å–µ–≥–æ–¥–Ω—è—à–Ω–µ–º—É –¥–Ω—é."""
    try:
        dt = datetime.fromisoformat(published_at.replace("Z", "+00:00"))
        return dt.date() == date.today()
    except Exception:
        return False


def collect_from_source(src):
    """–°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ —Å–µ–≥–æ–¥–Ω—è –∏–∑ –æ–¥–Ω–æ–≥–æ RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–∞."""
    collected = []
    try:
        items = fetch_rss(src)
    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {src}: {e}")
        return collected

    if not items:
        log.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π RSS: {src}")
        return collected

    today_items = [n for n in items if is_today(n.get("published_at", ""))]
    log.info(f"üì° {src} ‚Üí –Ω–∞–π–¥–µ–Ω–æ {len(today_items)} –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è")

    for news in today_items:
        url = news.get("url")
        if not url:
            continue

        title = news.get("title", "").strip()
        summary = news.get("summary", "")
        published = news.get("published_at") or datetime.utcnow().isoformat()
        source = news.get("source", src)
        news_id = generate_id(url)

        # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        img_url = safe_fetch_image(url)
        img_path = None
        if img_url:
            img_path = download_image(img_url, IMG_DIR, news_id)
            if img_path:
                log.info(f"üñº {title[:60]}... ‚Äî –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ")
            else:
                log.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {title}")
        else:
            log.warning(f"‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {title}")

        collected.append({
            "id": news_id,
            "title": title,
            "url": url,
            "summary": summary[:600],
            "source": source,
            "published_at": published,
            "image_path": str(img_path) if img_path else None,
        })

    return collected


def save_to_json(items):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ JSON —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏."""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    IMG_DIR.mkdir(parents=True, exist_ok=True)

    output = {
        "collected_at": datetime.now(timezone.utc).isoformat(),
        "total": len(items),
        "items": items,
    }

    with open(NEWS_PATH, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    log.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(items)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ {NEWS_PATH.resolve()}")


def collect_all():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: —Å–±–æ—Ä —Ç–æ–ª—å–∫–æ —Å–µ–≥–æ–¥–Ω—è—à–Ω–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π."""
    log.info("üöÄ –°—Ç–∞—Ä—Ç —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è")
    existing_ids = load_existing_ids()
    all_news = []

    for src in RSS_SOURCES:
        source_news = collect_from_source(src)
        for item in source_news:
            if item["id"] not in existing_ids:
                all_news.append(item)
                existing_ids.add(item["id"])

    random.shuffle(all_news)
    log.info(f"‚úÖ –ò—Ç–æ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(all_news)} –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è")
    save_to_json(all_news)
    build_schedule(len(all_news))
    return all_news