import json
import os
import re
import hashlib
import requests
from pathlib import Path
from urllib.parse import urlparse, urljoin
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup
from core.logger import log
from sources.rss import fetch_rss



# === –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ===
MAX_ITEMS_PER_SOURCE = 20
DATA_FILE = Path("data/news.json")
# === –ü–£–¢–ò ===
DATA_DIR = Path("data")
IMG_DIR = DATA_DIR / "images"     # ‚Üê –≤–æ—Ç —ç—Ç–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –î–û –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; ITNewsBot/1.0)"}

RSS_SOURCES = [
    "https://www.theverge.com/rss/index.xml",
    "https://www.wired.com/feed/rss",
    "https://techcrunch.com/feed/",
    "https://feeds.arstechnica.com/arstechnica/index",
    "https://www.engadget.com/rss.xml",
    "https://venturebeat.com/feed/",
    "https://github.blog/feed/",
    "https://stackoverflow.blog/feed/",
    "https://feeds.feedburner.com/TheHackersNews",
]

WHITE_DOMAINS = [
    "theverge.com", "techcrunch.com", "wired.com", "arstechnica.com",
    "venturebeat.com", "github.blog", "engadget.com", "stackoverflow.blog",
    "feeds.feedburner.com"
]

BAD_KEYWORDS = [
    "discount", "sale", "offer", "affiliate", "casino", "bet", "token",
    "crypto", "sponsored", "vpn", "deal", "price"
]


# === –§–∏–ª—å—Ç—Ä—ã ===
def is_trusted_source(url: str) -> bool:
    domain = urlparse(url).netloc.lower()
    return any(d in domain for d in WHITE_DOMAINS)

def has_bad_words(text: str) -> bool:
    return any(bad in (text or "").lower() for bad in BAD_KEYWORDS)

def _is_recent_day(published_at, days=2):
    try:
        if isinstance(published_at, str):
            published_at = datetime.fromisoformat(published_at[:19])
    except Exception:
        return False
    return published_at >= datetime.utcnow() - timedelta(days=days)
def _dedup_by_url(items):
    """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏ –ø–æ –ø–æ–ª—é url"""
    seen = set()
    result = []
    for it in items:
        url = it.get("url")
        if url and url not in seen:
            seen.add(url)
            result.append(it)
    return result
def is_valid_title(title: str) -> bool:
    wc = len(title.split())
    return 3 <= wc <= 15


# === –£—Ç–∏–ª–∏—Ç—ã ===
def generate_id(url: str) -> str:
    return hashlib.sha1(url.encode("utf-8")).hexdigest()[:10]

def fetch_main_image(url):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—É—é –∫–∞—Ä—Ç–∏–Ω–∫—É —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        resp.raise_for_status()
    except Exception as e:
        log.warning(f"[fetch_main_image] Failed to fetch {url}: {e}")
        return None

    soup = BeautifulSoup(resp.text, "html.parser")

    for prop in ["og:image", "twitter:image", "twitter:image:src"]:
        tag = soup.find("meta", property=prop) or soup.find("meta", attrs={"name": prop})
        if tag and tag.get("content"):
            return urljoin(url, tag["content"])

    link_tag = soup.find("link", rel="image_src")
    if link_tag and link_tag.get("href"):
        return urljoin(url, link_tag["href"])

    article = soup.find("article") or soup.find("main") or soup
    img_tag = article.find("img")
    if img_tag and img_tag.get("src") and not img_tag["src"].startswith("data:"):
        return urljoin(url, img_tag["src"])

    return None

def download_image(image_url: str, folder: Path, news_id: str):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫—É –∏ –∫—ç—à–∏—Ä—É–µ—Ç –µ—ë."""
    ext = os.path.splitext(urlparse(image_url).path)[1]
    if not ext or len(ext) > 6:
        ext = ".jpg"

    img_path = folder / f"preview_{news_id}{ext}"
    if img_path.exists():
        log.info(f"üü° Cached: {img_path.name}")
        return str(img_path)

    try:
        resp = requests.get(image_url, headers=HEADERS, timeout=10)
        resp.raise_for_status()
        with open(img_path, "wb") as f:
            f.write(resp.content)
        log.info(f"üñº Saved new image: {img_path}")
        return str(img_path)
    except Exception as e:
        log.warning(f"‚ö†Ô∏è Failed to download image {image_url}: {e}")
        return None


# === –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ===
def collect_all():
    """
    –°–æ–±–∏—Ä–∞–µ—Ç —Å–≤–µ–∂–∏–µ —Å—Ç–∞—Ç—å–∏ –∏–∑ –¥–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ data/news.json,
    —Å—Ç—Ä–æ–≥–æ –≤ –ø—Ä–µ–∂–Ω–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –ø–æ–ª–µ–π (id, title, url, summary, source, published_at, image_path).
    """
    IMG_DIR.mkdir(parents=True, exist_ok=True)
    os.makedirs("data", exist_ok=True)

    collected = []

    for src in RSS_SOURCES:
        try:
            items = fetch_rss(src, limit=MAX_ITEMS_PER_SOURCE)
            log.info(f"Fetched {len(items)} from {src}")
        except Exception as e:
            log.error(f"Error fetching {src}: {e}")
            continue

        # –µ—Å–ª–∏ –∑–∞–¥–∞–Ω –ª–∏–º–∏—Ç ‚Äî –ø–æ–¥—Å—Ç—Ä–∞—Ö—É–µ–º—Å—è –µ—â—ë —Ä–∞–∑
        sliced = items if MAX_ITEMS_PER_SOURCE is None else items[:MAX_ITEMS_PER_SOURCE]

        for news in sliced:
            url = news.get("url")
            title = news.get("title", "").strip()
            published = (news.get("published_at") or datetime.utcnow().replace(tzinfo=timezone.utc).isoformat())

            # —Ñ–∏–ª—å—Ç—Ä—ã –∫–∞—á–µ—Å—Ç–≤–∞/–≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ ‚Äî –∫–∞–∫ —É —Ç–µ–±—è –±—ã–ª–æ
            if not url:
                continue
            if not is_trusted_source(url):
                continue
            if has_bad_words(title):
                continue
            if not is_valid_title(title):
                continue
            if not _is_recent_day(published, days=7):  # –¥–æ–ø—É—Å—Ç–∏–º, –Ω–µ —Å—Ç–∞—Ä—à–µ –Ω–µ–¥–µ–ª–∏
                continue

            news_id = generate_id(url)

            # –∫–∞—Ä—Ç–∏–Ω–∫–∞ –æ—Å—Ç–∞—ë—Ç—Å—è –∫–∞–∫ –±—ã–ª–æ (–º–æ–∂–Ω–æ –æ—Ç–∫–ª—é—á–∏—Ç—å, –µ—Å–ª–∏ –Ω–µ –Ω—É–∂–Ω–æ)
            saved_img = None
            try:
                img_url = fetch_main_image(url)
                if img_url:
                    saved_img = download_image(img_url, IMG_DIR, news_id)
            except Exception:
                saved_img = None

            # –í–ê–ñ–ù–û: –ø–∏—à–µ–º —Å—Ç—Ä–æ–≥–æ –ø—Ä–µ–∂–Ω–∏–µ –ø–æ–ª—è
            collected.append({
                "id": news_id,
                "title": title,
                "url": url,
                "summary": news.get("summary", ""),
                "source": news.get("source") or src,
                "published_at": published,
                "image_path": saved_img  # –º–æ–∂–µ—Ç –±—ã—Ç—å None
            })

    # –¥–µ–¥—É–ø –ø–æ URL –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ (–Ω–æ–≤—ã–µ —Å–≤–µ—Ä—Ö—É)
    collected = _dedup_by_url(collected)
    try:
        collected.sort(key=lambda x: x.get("published_at", ""), reverse=True)
    except Exception:
        pass

    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    with open(DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(collected, f, ensure_ascii=False, indent=2)

    log.info(f"‚úÖ Saved {len(collected)} items to {DATA_FILE.resolve()}")
    return collected
